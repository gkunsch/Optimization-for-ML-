{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: implement the relu function and its VJP in the format above. Using the finite difference equation (slide 13), make sure that the VJP is correct numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x: an array\n",
    "\n",
    "  Returns:\n",
    "    - value of the function ReLU(x) \n",
    "    - function vjp to easily compute vjp of ReLU\n",
    "  \"\"\"\n",
    "  value = np.maximum(x,0) #by defintion of the ReLU function \n",
    "  \n",
    "  def vjp(u):\n",
    "    relu_derivative = (x > 0) * 1 #by definition of the derivative and ReLU function \n",
    "    vjp_wrt_x = np.multiply(u,relu_derivative) #using slides 27 of the course\n",
    "    return vjp_wrt_x,  \n",
    "\n",
    "  return value, vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create some functions to numerically check if the function defined above (and others ones later) are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vjp(f, x, u, eps=1e-3):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    f: a function returning an array.\n",
    "    x: an array of size n \n",
    "    eps: numerical value (very small)\n",
    "    u: an array of size m \n",
    "\n",
    "  Returns:\n",
    "    numerical_vjp\n",
    "  \"\"\"\n",
    "  \n",
    "  def e(i): # to define each direction in the space\n",
    "    basis_vector = np.zeros(len(x))\n",
    "    basis_vector[i] = 1\n",
    "    return basis_vector\n",
    "  \n",
    "  Jacobian = np.zeros((len(f(x)),len(x)))\n",
    "  for i in range (len(x)):\n",
    "    Jacobian[:,i] = (f(x + eps * e(i)) - f(x)) / eps #finite difference\n",
    "  \n",
    "  return np.dot(Jacobian.T,u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then test our implemented function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numeric method gives for the VJP:\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -1.41025641 -1.23076923 -1.05128205 -0.87179487\n",
      " -0.69230769 -0.51282051 -0.33333333 -0.15384615  0.02564103  0.20512821\n",
      "  0.38461538  0.56410256  0.74358974  0.92307692  1.1025641   1.28205128\n",
      "  1.46153846  1.64102564  1.82051282  2.        ]\n"
     ]
    }
   ],
   "source": [
    "# we define some values for x and u, they must same dimension because for ReLU space of inputs and outputs are the same\n",
    "x = np.linspace(start = -2, stop = 2, num = 40)\n",
    "u = np.linspace(start = -5, stop = 2, num = 40)\n",
    "\n",
    "def relu_function(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "relu_numerical_vjp = test_vjp(relu_function, x, u, eps=1e-3)\n",
    "implemented_vjp = relu(x)[1](u)[0]\n",
    "\n",
    "print(\"The numeric method gives for the VJP:\")\n",
    "print(relu_numerical_vjp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The implemented method gives for the VJP:\n",
      "[-0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -1.41025641 -1.23076923 -1.05128205 -0.87179487\n",
      " -0.69230769 -0.51282051 -0.33333333 -0.15384615  0.02564103  0.20512821\n",
      "  0.38461538  0.56410256  0.74358974  0.92307692  1.1025641   1.28205128\n",
      "  1.46153846  1.64102564  1.82051282  2.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"The implemented method gives for the VJP:\")\n",
    "print(implemented_vjp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two methods give the same results indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: reusing dot and relu, implement a 2-layer MLP with a relu activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we import dot from the google doc\n",
    "def dot(W, x):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    W: an a matrix of shape (n1,n)\n",
    "    x: an array of shape n \n",
    "\n",
    "  Returns:\n",
    "    - value of the function dot(W,x) \n",
    "    - function vjp to easily compute vjp of the dot product\n",
    "  \"\"\"\n",
    "  value = np.dot(W, x)\n",
    "\n",
    "  def vjp(u):\n",
    "    return np.outer(u, x), np.dot(W.T,u)\n",
    "\n",
    "  return value, vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp2(x, W1, W2):\n",
    "    \"\"\"\n",
    "    This function defines a MLP-2 architecure with ReLU function activation\n",
    "\n",
    "    Args:\n",
    "        x: an array of shape n \n",
    "        W1: an a matrix of shape (n1,n)\n",
    "        W2: an a matrix of shape (1,n1)\n",
    "\n",
    "    Returns:\n",
    "        - value of the function mlp2(x, W1, W2)\n",
    "        - function vjp to easily compute vjp of mlp2 archietcture \n",
    "    \"\"\"\n",
    "\n",
    "    x1 = dot(W1,x)[0] #1st linear layer \n",
    "    x2 = relu(x1)[0] #ReLU activation \n",
    "    x3 = dot(W2,x2)[0] #2nd linear layer\n",
    "    value = x3\n",
    "\n",
    "    def vjp(u):\n",
    "        \n",
    "        vjp_jacobian_dot_W2 = dot(W2,x2)[1](u) \n",
    "        vjp_wrt_W2 = vjp_jacobian_dot_W2[0] # we keep the 1st vjp, with respect to W2\n",
    "\n",
    "        vjp_jacobian_dot_W2_wrt_x2 = vjp_jacobian_dot_W2[1] # we keep the 2nd vjp, with respect to x2\n",
    "        vjp_jacobian_relu_wrt_x1 = relu(x1)[1](vjp_jacobian_dot_W2_wrt_x2)[0]\n",
    "\n",
    "        vjp_jacobian_dot_W1 =  dot(W1,x)[1](vjp_jacobian_relu_wrt_x1) \n",
    "        vjp_wrt_W1 = vjp_jacobian_dot_W1[0] # we keep the 1st vjp, with respect to W1\n",
    "        vjp_wrt_x = vjp_jacobian_dot_W1[1] # we keep the 2nd vjp, with respect to x\n",
    "        \n",
    "        return vjp_wrt_x,vjp_wrt_W1, vjp_wrt_W2\n",
    "\n",
    "    return value, vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: implement the squared loss VJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_pred, y):\n",
    "    \"\"\"\n",
    "    This function defines squared-loss between 2 vectors\n",
    "\n",
    "    Args:\n",
    "        y_pred: a scalar\n",
    "        y: a scalar \n",
    "        \n",
    "    Returns:\n",
    "        - value of the function squared_loss(y_pred, y)\n",
    "        - function vjp to easily compute vjp of the squared loss\n",
    "    \"\"\"\n",
    "\n",
    "    residual = y_pred - y\n",
    "    value = 0.5 * np.sum(residual ** 2)\n",
    "\n",
    "    def vjp(u):\n",
    "        vjp_y_pred = np.multiply(residual,u)\n",
    "        vjp_y = -np.multiply(residual,u)\n",
    "        return vjp_y_pred, vjp_y\n",
    "    \n",
    "    # The code requires every output to be an array.\n",
    "    return np.array([value]), vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: implement the loss by composing mlp2 and squared_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y, W1, W2):\n",
    "    \"\"\"\n",
    "    This function defines the loss, by combining previous function squared_loss and mlp2\n",
    "\n",
    "    Args:\n",
    "        x: an array of shape n \n",
    "        y: an array of shape n \n",
    "        W1: an a matrix of shape (n1,n)\n",
    "        W2: an a matrix of shape (1,n1)\n",
    "        \n",
    "    Returns:\n",
    "        - value of the function loss(x, y, W1, W2)\n",
    "        - function vjp to easily compute vjp of the loss\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = mlp2(x, W1, W2)[0]\n",
    "    value = squared_loss(y_pred, y)[0]\n",
    "\n",
    "    def vjp(u):\n",
    "        \n",
    "        vjp_squared_loss = squared_loss(y_pred, y)[1](u)\n",
    "        vjp_wrt_y = vjp_squared_loss[1] # we keep the vjp with respect to y \n",
    "\n",
    "        vjp_mlp2 = mlp2(x, W1, W2)[1](vjp_squared_loss[0])\n",
    "\n",
    "        vjp_wrt_W2 = vjp_mlp2[2] # we keep the vjp with respect to W2\n",
    "        vjp_wrt_W1 = vjp_mlp2[1] # we keep the vjp with respect to W1\n",
    "        vjp_wrt_x = vjp_mlp2[0] # we keep the vjp with respect to Wx\n",
    "        \n",
    "        return vjp_wrt_x, vjp_wrt_y, vjp_wrt_W1, vjp_wrt_W2\n",
    "\n",
    "    return value, vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's numerically test our architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the gradient for x coordinate \n",
    "\n",
    "def test_architecture_vjp(f,x,y,W1,W2,u, eps=1e-3): \n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        f: a function returning an array.\n",
    "        x: an array of size n \n",
    "        eps: numerical value (very small)\n",
    "        u: an array of size m \n",
    "\n",
    "    Returns:\n",
    "        numerical_vjp\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def e(i): # to define each direction in the space\n",
    "        basis_vector = np.zeros(len(x))\n",
    "        basis_vector[i] = 1\n",
    "        return basis_vector\n",
    "  \n",
    "    Jacobian = np.zeros((len(f(x,y,W1,W2)),len(x)))\n",
    "    for i in range (len(x)):\n",
    "        Jacobian[:,i] = (f(x + eps * e(i),y,W1,W2) - f(x,y,W1,W2)) / eps #finite difference\n",
    "  \n",
    "    return np.dot(Jacobian.T,u) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numeric method gives for the VJP:\n",
      "[ 3.45052383  2.66317577  8.78690297  5.96159959  4.42238155  9.11140057\n",
      "  6.82954782  3.20253968 12.33649434  3.85880904  4.85426372  6.61494078\n",
      "  2.53303104  5.06409083  7.5592683  10.02295287  8.24660279  7.46863938\n",
      "  8.20774187  9.41294542  5.28011901  8.09393933  2.29111948  7.73468241\n",
      "  9.47269964  2.09238798  6.47378223  5.19637709  8.21736629  5.89765486\n",
      "  6.15299244  5.86131837  9.17960146  9.3161047   7.34579066  9.54217488\n",
      "  7.17141859  5.87332171  9.05419728  3.43960389]\n"
     ]
    }
   ],
   "source": [
    "# we define some values for x and u, they must same dimension because for ReLU space of inputs and outputs are the same\n",
    "m = 5\n",
    "k = 10\n",
    "x = np.linspace(start = -2, stop = 2, num = 40)\n",
    "u = np.random.rand(1) #size 1 beacuse f is in value of scalar\n",
    "W1 = np.random.rand(m,len(x))\n",
    "W2 = np.random.rand(k,m)\n",
    "y = np.linspace(start = -2, stop = 2, num = k)\n",
    "\n",
    "\n",
    "def mlp2_function(x,y,W1,W2):\n",
    "    x1 = dot(W1,x)[0] #1st linear layer \n",
    "    x2 = relu(x1)[0] #ReLU activation \n",
    "    y_pred = dot(W2,x2)[0] #2nd linear layer\n",
    "    residual = y_pred - y\n",
    "    value = 0.5 * np.sum(residual ** 2)\n",
    "    return np.array([value])\n",
    "\n",
    "relu_numerical_vjp = test_architecture_vjp(mlp2_function,x,y,W1,W2,u, eps=1e-3)\n",
    "implemented_vjp = relu(x)[1](u)[0]\n",
    "\n",
    "print(\"The numeric method gives for the VJP:\")\n",
    "print(relu_numerical_vjp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
