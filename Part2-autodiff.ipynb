{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: implement the relu function and its VJP in the format above. Using the finite difference equation (slide 13), make sure that the VJP is correct numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x: an array\n",
    "\n",
    "  Returns:\n",
    "    - value of the function ReLU(x) \n",
    "    - function vjp to easily compute vjp of ReLU\n",
    "  \"\"\"\n",
    "  value = np.maximum(x,0) #by defintion of the ReLU function \n",
    "  \n",
    "  def vjp(u):\n",
    "    relu_derivative = (x > 0) * 1 #by definition of the derivative and ReLU function \n",
    "    vjp_wrt_x = np.multiply(u,relu_derivative) #using slides 27 of the course\n",
    "    return vjp_wrt_x,  \n",
    "\n",
    "  return value, vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create some functions to numerically check if the function defined above (and others ones later) are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vjp(f, x, u, eps=1e-3):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    f: a function returning an array.\n",
    "    x: an array of size n \n",
    "    eps: numerical value (very small)\n",
    "    u: an array of size m \n",
    "\n",
    "  Returns:\n",
    "    numerical_vjp\n",
    "  \"\"\"\n",
    "  \n",
    "  def e(i): # to define each direction in the space\n",
    "    basis_vector = np.zeros(len(x))\n",
    "    basis_vector[i] = 1\n",
    "    return basis_vector\n",
    "  \n",
    "  Jacobian = np.zeros((len(f(x)),len(x)))\n",
    "  for i in range (len(x)):\n",
    "    Jacobian[:,i] = (f(x + eps * e(i)) - f(x)) / eps #finite difference\n",
    "  \n",
    "  return np.dot(Jacobian.T,u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then test our implemented function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numeric method gives for the VJP:\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -1.41025641 -1.23076923 -1.05128205 -0.87179487\n",
      " -0.69230769 -0.51282051 -0.33333333 -0.15384615  0.02564103  0.20512821\n",
      "  0.38461538  0.56410256  0.74358974  0.92307692  1.1025641   1.28205128\n",
      "  1.46153846  1.64102564  1.82051282  2.        ]\n"
     ]
    }
   ],
   "source": [
    "# we define some values for x and u, they must same dimension because for ReLU space of inputs and outputs are the same\n",
    "x = np.linspace(start = -2, stop = 2, num = 40)\n",
    "u = np.linspace(start = -5, stop = 2, num = 40)\n",
    "\n",
    "def relu_function(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "relu_numerical_vjp = test_vjp(relu_function, x, u, eps=1e-3)\n",
    "implemented_vjp = relu(x)[1](u)[0]\n",
    "\n",
    "print(\"The numeric method gives for the VJP:\")\n",
    "print(relu_numerical_vjp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The implemented method gives for the VJP:\n",
      "[-0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -1.41025641 -1.23076923 -1.05128205 -0.87179487\n",
      " -0.69230769 -0.51282051 -0.33333333 -0.15384615  0.02564103  0.20512821\n",
      "  0.38461538  0.56410256  0.74358974  0.92307692  1.1025641   1.28205128\n",
      "  1.46153846  1.64102564  1.82051282  2.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"The implemented method gives for the VJP:\")\n",
    "print(implemented_vjp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two methods give the same results indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: reusing dot and relu, implement a 2-layer MLP with a relu activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we import dot from the google doc\n",
    "def dot(W, x):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    W: an a matrix of shape (n1,n)\n",
    "    x: an array of shape n \n",
    "\n",
    "  Returns:\n",
    "    - value of the function dot(W,x) \n",
    "    - function vjp to easily compute vjp of the dot product\n",
    "  \"\"\"\n",
    "  value = np.dot(W, x)\n",
    "\n",
    "  def vjp(u):\n",
    "    return np.outer(u, x), np.dot(W.T,u)\n",
    "\n",
    "  return value, vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp2(x, W1, W2):\n",
    "    \"\"\"\n",
    "    This function defines a MLP-2 architecure with ReLU function activation\n",
    "\n",
    "    Args:\n",
    "        x: an array of shape n \n",
    "        W1: an a matrix of shape (n1,n)\n",
    "        W2: an a matrix of shape (1,n1)\n",
    "\n",
    "    Returns:\n",
    "        - value of the function mlp2(x, W1, W2)\n",
    "        - function vjp to easily compute vjp of mlp2 archietcture \n",
    "    \"\"\"\n",
    "\n",
    "    x1 = dot(W1,x)[0] #1st linear layer \n",
    "    x2 = relu(x1)[0] #ReLU activation \n",
    "    x3 = dot(W2,x2)[0] #2nd linear layer\n",
    "    value = x3\n",
    "\n",
    "    def vjp(u):\n",
    "        \n",
    "        vjp_jacobian_dot_W2 = dot(W2,x2)[1](u) \n",
    "        vjp_wrt_W2 = vjp_jacobian_dot_W2[0] # we keep the 1st vjp, with respect to W2\n",
    "\n",
    "        vjp_jacobian_dot_W2_wrt_x2 = vjp_jacobian_dot_W2[1] # we keep the 2nd vjp, with respect to x2\n",
    "        vjp_jacobian_relu_wrt_x1 = relu(x1)[1](vjp_jacobian_dot_W2_wrt_x2)[0]\n",
    "\n",
    "        vjp_jacobian_dot_W1 =  dot(W1,x)[1](vjp_jacobian_relu_wrt_x1) \n",
    "        vjp_wrt_W1 = vjp_jacobian_dot_W1[0] # we keep the 1st vjp, with respect to W1\n",
    "        vjp_wrt_x = vjp_jacobian_dot_W1[1] # we keep the 2nd vjp, with respect to x\n",
    "        \n",
    "        return vjp_wrt_x,vjp_wrt_W1, vjp_wrt_W2\n",
    "\n",
    "    return value, vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: implement the squared loss VJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_pred, y):\n",
    "    \"\"\"\n",
    "    This function defines squared-loss between 2 vectors\n",
    "\n",
    "    Args:\n",
    "        y_pred: a scalar\n",
    "        y: a scalar \n",
    "        \n",
    "    Returns:\n",
    "        - value of the function squared_loss(y_pred, y)\n",
    "        - function vjp to easily compute vjp of the squared loss\n",
    "    \"\"\"\n",
    "\n",
    "    residual = y_pred - y\n",
    "    value = 0.5 * np.sum(residual ** 2)\n",
    "\n",
    "    def vjp(u):\n",
    "        vjp_y_pred = np.multiply(residual,u)\n",
    "        vjp_y = -np.multiply(residual,u)\n",
    "        return vjp_y_pred, vjp_y\n",
    "    \n",
    "    # The code requires every output to be an array.\n",
    "    return np.array([value]), vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: implement the loss by composing mlp2 and squared_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y, W1, W2):\n",
    "    \"\"\"\n",
    "    This function defines the loss, by combining previous function squared_loss and mlp2\n",
    "\n",
    "    Args:\n",
    "        x: an array of shape n \n",
    "        y: an array of shape n \n",
    "        W1: an a matrix of shape (n1,n)\n",
    "        W2: an a matrix of shape (1,n1)\n",
    "        \n",
    "    Returns:\n",
    "        - value of the function loss(x, y, W1, W2)\n",
    "        - function vjp to easily compute vjp of the loss\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = mlp2(x, W1, W2)[0]\n",
    "    value = squared_loss(y_pred, y)[0]\n",
    "\n",
    "    def vjp(u):\n",
    "        \n",
    "        vjp_squared_loss = squared_loss(y_pred, y)[1](u)\n",
    "        vjp_wrt_y = vjp_squared_loss[1] # we keep the vjp with respect to y \n",
    "\n",
    "        vjp_mlp2 = mlp2(x, W1, W2)[1](vjp_squared_loss[0])\n",
    "\n",
    "        vjp_wrt_W2 = vjp_mlp2[2] # we keep the vjp with respect to W2\n",
    "        vjp_wrt_W1 = vjp_mlp2[1] # we keep the vjp with respect to W1\n",
    "        vjp_wrt_x = vjp_mlp2[0] # we keep the vjp with respect to Wx\n",
    "        \n",
    "        return vjp_wrt_x, vjp_wrt_y, vjp_wrt_W1, vjp_wrt_W2\n",
    "\n",
    "    return value, vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's numerically test our architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the gradient for x coordinate \n",
    "\n",
    "def test_architecture_vjp(f,x,y,W1,W2,u, eps=1e-3): \n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        f: a function returning an array.\n",
    "        x: an array of size n \n",
    "        eps: numerical value (very small)\n",
    "        u: an array of size m \n",
    "\n",
    "    Returns:\n",
    "        numerical_vjp\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def e(i): # to define each direction in the space\n",
    "        basis_vector = np.zeros(len(x))\n",
    "        basis_vector[i] = 1\n",
    "        return basis_vector\n",
    "  \n",
    "    Jacobian = np.zeros((len(f(x,y,W1,W2)),len(x)))\n",
    "    for i in range (len(x)):\n",
    "        Jacobian[:,i] = (f(x + eps * e(i),y,W1,W2) - f(x,y,W1,W2)) / eps #finite difference\n",
    "  \n",
    "    return np.dot(Jacobian.T,u) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numeric method gives for the VJP:\n",
      "[19.14309305 31.02038373 13.551424   18.78970642 14.00509983  9.56020723\n",
      " 14.71274905 11.31055246  8.55884337 22.08319552 22.84754325 17.30162419\n",
      " 17.64021487 10.82238194 18.82459114 21.00685297 17.64828929 14.03794649\n",
      " 15.05615251 20.16819522 22.39873588 24.79450688 25.21108178 19.44744371\n",
      " 22.84021285 11.48090129 18.28434479 22.93798924 20.68969653 24.62624397\n",
      " 17.00913949 23.42523998 16.74358294 20.13131055 12.06350053 23.43086203\n",
      " 19.3793132  19.84783173 17.18354963 18.41176224]\n"
     ]
    }
   ],
   "source": [
    "# we define some values for x and u, they must same dimension because for ReLU space of inputs and outputs are the same\n",
    "m = 5\n",
    "k = 10\n",
    "x = np.linspace(start = -2, stop = 2, num = 40)\n",
    "u = np.random.rand(1) #size 1 beacuse f is in value of scalar\n",
    "W1 = np.random.rand(m,len(x))\n",
    "W2 = np.random.rand(k,m)\n",
    "y = np.linspace(start = -2, stop = 2, num = k)\n",
    "\n",
    "\n",
    "def mlp2_function(x,y,W1,W2):\n",
    "    x1 = dot(W1,x)[0] #1st linear layer \n",
    "    x2 = relu(x1)[0] #ReLU activation \n",
    "    y_pred = dot(W2,x2)[0] #2nd linear layer\n",
    "    residual = y_pred - y\n",
    "    value = 0.5 * np.sum(residual ** 2)\n",
    "    return np.array([value])\n",
    "\n",
    "relu_numerical_vjp = test_architecture_vjp(mlp2_function,x,y,W1,W2,u, eps=1e-3)\n",
    "implemented_vjp = loss(x, y, W1, W2)[1](u)[0]\n",
    "\n",
    "print(\"The numeric method gives for the VJP:\")\n",
    "print(relu_numerical_vjp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The implemented method gives for the VJP:\n",
      "[19.13906221 31.00964259 13.54898279 18.78506355 14.00298886  9.55922614\n",
      " 14.70971739 11.30888588  8.55778253 22.07716111 22.84145503 17.29758806\n",
      " 17.6363592  10.82089584 18.82030412 21.00178511 17.64491321 14.03518706\n",
      " 15.05330498 20.16371112 22.39282713 24.7873072  25.2036321  19.44281785\n",
      " 22.83448492 11.47945465 18.28035723 22.93147033 20.68446617 24.61932994\n",
      " 17.00594287 23.41844296 16.74014244 20.12635058 12.06147861 23.42470829\n",
      " 19.37510905 19.84274023 17.18000262 18.40803846]\n"
     ]
    }
   ],
   "source": [
    "print(\"The implemented method gives for the VJP:\")\n",
    "print(implemented_vjp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 methods are really close but don't give exactly same results (this is because of numerical approximations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
