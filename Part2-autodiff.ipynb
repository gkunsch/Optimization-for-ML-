{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: implement the relu function and its VJP in the format above. Using the finite difference equation (slide 13), make sure that the VJP is correct numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x: an array\n",
    "\n",
    "  Returns:\n",
    "    - value of the function ReLU(x) \n",
    "    - function vjp to easily compute vjp of ReLU\n",
    "  \"\"\"\n",
    "  value = np.maximum(x,0) #by defintion of the ReLU function \n",
    "  \n",
    "  def vjp(u):\n",
    "    relu_derivative = (x > 0) * 1 #by definition of the derivative and ReLU function \n",
    "    vjp_wrt_x = np.multiply(u,relu_derivative) #using slides 27 of the course\n",
    "    return vjp_wrt_x,  \n",
    "\n",
    "  return value, vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create some functions to numerically check if the function defined above (and others ones later) are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vjp(f, x, u, eps=1e-3):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    f: a function returning a tuple of size 2: array and vjp\n",
    "    x: an array of size n \n",
    "    eps: numerical value (very small)\n",
    "    u: an array of size m \n",
    "\n",
    "  Returns:\n",
    "    numerical_vjp\n",
    "  \"\"\"\n",
    "  \n",
    "  def e(i): # to define each direction in the space\n",
    "    basis_vector = np.zeros(len(x))\n",
    "    basis_vector[i] = 1\n",
    "    return basis_vector\n",
    "  \n",
    "  Jacobian = np.zeros((len(f(x)[0]),len(x)))\n",
    "  for i in range (len(x)):\n",
    "    Jacobian[:,i] = (f(x + eps * e(i))[0] - f(x)[0]) / eps #finite difference\n",
    "  \n",
    "  return np.dot(Jacobian.T,u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then test our implemented function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numeric method gives for the VJP:\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -1.41025641 -1.23076923 -1.05128205 -0.87179487\n",
      " -0.69230769 -0.51282051 -0.33333333 -0.15384615  0.02564103  0.20512821\n",
      "  0.38461538  0.56410256  0.74358974  0.92307692  1.1025641   1.28205128\n",
      "  1.46153846  1.64102564  1.82051282  2.        ]\n"
     ]
    }
   ],
   "source": [
    "# we define some values for x and u, they must same dimension because for ReLU space of inputs and outputs are the same\n",
    "x = np.linspace(start = -2, stop = 2, num = 40)\n",
    "u = np.linspace(start = -5, stop = 2, num = 40)\n",
    "\n",
    "implemented_vjp = relu(x)[1](u)[0]\n",
    "relu_numerical_vjp = test_vjp(relu, x, u, eps=1e-3)\n",
    "\n",
    "print(\"The numeric method gives for the VJP:\")\n",
    "print(relu_numerical_vjp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The implemented method gives for the VJP:\n",
      "[-0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -1.41025641 -1.23076923 -1.05128205 -0.87179487\n",
      " -0.69230769 -0.51282051 -0.33333333 -0.15384615  0.02564103  0.20512821\n",
      "  0.38461538  0.56410256  0.74358974  0.92307692  1.1025641   1.28205128\n",
      "  1.46153846  1.64102564  1.82051282  2.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"The implemented method gives for the VJP:\")\n",
    "print(implemented_vjp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two methods give the same results indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: reusing dot and relu, implement a 2-layer MLP with a relu activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we import dot from the google doc\n",
    "def dot(W, x):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    W: an a matrix of shape (n1,n)\n",
    "    x: an array of shape n \n",
    "\n",
    "  Returns:\n",
    "    - value of the function dot(W,x) \n",
    "    - function vjp to easily compute vjp of the dot product\n",
    "  \"\"\"\n",
    "  value = np.dot(W, x)\n",
    "\n",
    "  def vjp(u):\n",
    "    return np.outer(u, x), np.dot(W.T,u)\n",
    "\n",
    "  return value, vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp2(x, W1, W2):\n",
    "    \"\"\"\n",
    "    This function defines a MLP-2 architecure with ReLU function activation\n",
    "\n",
    "    Args:\n",
    "        x: an array of shape n \n",
    "        W1: an a matrix of shape (n1,n)\n",
    "        W2: an a matrix of shape (1,n1)\n",
    "\n",
    "    Returns:\n",
    "        - value of the function mlp2(x, W1, W2)\n",
    "        - function vjp to easily compute vjp of mlp2 archietcture \n",
    "    \"\"\"\n",
    "\n",
    "    x1 = dot(W1,x)[0] #1st linear layer \n",
    "    x2 = relu(x1)[0] #ReLU activation \n",
    "    x3 = dot(W2,x2)[0] #2nd linear layer\n",
    "    value = x3\n",
    "\n",
    "    def vjp(u):\n",
    "        \n",
    "        vjp_jacobian_dot_W2 = dot(W2,x2)[1](u) \n",
    "        vjp_wrt_W2 = vjp_jacobian_dot_W2[0] # we keep the 1st vjp, with respect to W2\n",
    "\n",
    "        vjp_jacobian_dot_W2_wrt_x2 = vjp_jacobian_dot_W2[1] # we keep the 2nd vjp, with respect to x2\n",
    "        vjp_jacobian_relu_wrt_x1 = relu(x1)[1](vjp_jacobian_dot_W2_wrt_x2)[0]\n",
    "\n",
    "        vjp_jacobian_dot_W1 =  dot(W1,x)[1](vjp_jacobian_relu_wrt_x1) \n",
    "        vjp_wrt_W1 = vjp_jacobian_dot_W1[0] # we keep the 1st vjp, with respect to W1\n",
    "        vjp_wrt_x = vjp_jacobian_dot_W1[1] # we keep the 2nd vjp, with respect to x\n",
    "        \n",
    "        return vjp_wrt_x,vjp_wrt_W1, vjp_wrt_W2\n",
    "\n",
    "    return value, vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: implement the squared loss VJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_pred, y):\n",
    "    \"\"\"\n",
    "    This function defines squared-loss between 2 vectors\n",
    "\n",
    "    Args:\n",
    "        y_pred: a scalar\n",
    "        y: a scalar \n",
    "        \n",
    "    Returns:\n",
    "        - value of the function squared_loss(y_pred, y)\n",
    "        - function vjp to easily compute vjp of the squared loss\n",
    "    \"\"\"\n",
    "\n",
    "    residual = y_pred - y\n",
    "    value = 0.5 * np.sum(residual ** 2)\n",
    "\n",
    "    def vjp(u):\n",
    "        vjp_y_pred = np.multiply(residual,u)\n",
    "        vjp_y = -np.multiply(residual,u)\n",
    "        return vjp_y_pred, vjp_y\n",
    "    \n",
    "    # The code requires every output to be an array.\n",
    "    return np.array([value]), vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: implement the loss by composing mlp2 and squared_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y, W1, W2):\n",
    "    \"\"\"\n",
    "    This function defines the loss, by combining previous function squared_loss and mlp2\n",
    "\n",
    "    Args:\n",
    "        x: an array of shape n \n",
    "        y: an array of shape n \n",
    "        W1: an a matrix of shape (n1,n)\n",
    "        W2: an a matrix of shape (1,n1)\n",
    "        \n",
    "    Returns:\n",
    "        - value of the function loss(x, y, W1, W2)\n",
    "        - function vjp to easily compute vjp of the loss\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = mlp2(x, W1, W2)[0]\n",
    "    value = squared_loss(y_pred, y)[0]\n",
    "\n",
    "    def vjp(u):\n",
    "        \n",
    "        vjp_squared_loss = squared_loss(y_pred, y)[1](u)\n",
    "        vjp_wrt_y = vjp_squared_loss[1] # we keep the vjp with respect to y \n",
    "\n",
    "        vjp_mlp2 = mlp2(x, W1, W2)[1](vjp_squared_loss[0])\n",
    "\n",
    "        vjp_wrt_W2 = vjp_mlp2[2] # we keep the vjp with respect to W2\n",
    "        vjp_wrt_W1 = vjp_mlp2[1] # we keep the vjp with respect to W1\n",
    "        vjp_wrt_x = vjp_mlp2[0] # we keep the vjp with respect to Wx\n",
    "        \n",
    "        return vjp_wrt_x, vjp_wrt_y, vjp_wrt_W1, vjp_wrt_W2\n",
    "\n",
    "    return value, vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's numerically test our architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the gradient for x coordinate \n",
    "\n",
    "def test_architecture_vjp(f,x,y,W1,W2,u, eps=1e-3): \n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        f: a function returning a tuple of size 2: array and vjp\n",
    "        x: an array of size n \n",
    "        eps: numerical value (very small)\n",
    "        u: an array of size m \n",
    "\n",
    "    Returns:\n",
    "        numerical_vjp\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def e(i): # to define each direction in the space\n",
    "        basis_vector = np.zeros(len(x))\n",
    "        basis_vector[i] = 1\n",
    "        return basis_vector\n",
    "  \n",
    "    Jacobian = np.zeros((len(f(x,y,W1,W2)[0]),len(x)))\n",
    "    for i in range (len(x)):\n",
    "        Jacobian[:,i] = (f(x + eps * e(i),y,W1,W2)[0] - f(x,y,W1,W2)[0]) / eps #finite difference\n",
    "  \n",
    "    return np.dot(Jacobian.T,u) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numeric method gives for the VJP:\n",
      "[ 7.4375586   9.87695888  1.95932956 18.31859134 13.76357548 13.58615237\n",
      "  5.81185748  8.99747236 13.11428322  5.4961817   3.79019975  8.97573862\n",
      " 12.07180389  7.88578523 15.63086431 11.85023676 12.93270159 10.13338122\n",
      " 14.74583103 12.95043222  9.4298893  12.08964973 10.6792685   2.82691115\n",
      " 14.01778692  9.70597254 13.69946318 14.59873402  2.86478384  7.94638825\n",
      " 12.4839964   7.11444291 14.73401608 10.34344895  4.92563418 11.89847156\n",
      "  9.02085114 17.81972381  8.43384488 17.17667414]\n"
     ]
    }
   ],
   "source": [
    "# we define some values for x and u, they must same dimension because for ReLU space of inputs and outputs are the same\n",
    "m = 5 #size \n",
    "k = 10 #size of the ouput vector for the MLP\n",
    "\n",
    "x = np.linspace(start = -2, stop = 2, num = 40) #size of the input vector\n",
    "y = np.linspace(start = -2, stop = 2, num = k)\n",
    "u = np.random.rand(1) #size 1 for vjp because f will output a scalar\n",
    "\n",
    "W1 = np.random.rand(m,len(x))\n",
    "W2 = np.random.rand(k,m)\n",
    "\n",
    "relu_numerical_vjp = test_architecture_vjp(loss,x,y,W1,W2,u, eps=1e-3)\n",
    "implemented_vjp = loss(x, y, W1, W2)[1](u)[0]\n",
    "\n",
    "print(\"The numeric method gives for the VJP:\")\n",
    "print(relu_numerical_vjp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The implemented method gives for the VJP:\n",
      "[ 7.43615383  9.87517025  1.95925226 18.31180555 13.75977471 13.58209663\n",
      "  5.81103095  8.9959864  13.11090335  5.49560483  3.78988702  8.97386563\n",
      " 12.06884202  7.88440531 15.62531862 11.84692928 12.92939966 10.13097778\n",
      " 14.74086667 12.94685423  9.42789153 12.08615381 10.67695728  2.82674093\n",
      " 14.0138359   9.70375019 13.6958312  14.59452164  2.86463335  7.94515883\n",
      " 12.48060479  7.11309926 14.72931155 10.34145929  4.92518079 11.89567071\n",
      "  9.01933432 17.81322035  8.43219516 17.17062616]\n"
     ]
    }
   ],
   "source": [
    "print(\"The implemented method gives for the VJP:\")\n",
    "print(implemented_vjp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 methods are really close but don't give exactly same results (this is because of numerical approximations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: implement an MLP with an arbitrary number of layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: implement SGD to train your MLP on a dataset of your choice. Study the impact of depth (number of layers) and width (number of hidden units).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement our autodiff on a simple dataset with a neural network using mlp2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(start = -2, stop = 2, num = 40)\n",
    "noise = np.random.rand(40)/10\n",
    "y = 3*x+2 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "965fe59137d5334f034bf4d21dcfaab0895405927d7ec8dd4d8796ddf51dcb72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
